{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743f3c49-5f1d-4132-bd0a-35daa53a0a48",
   "metadata": {},
   "source": [
    "Installing and importing relevant dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bc75f-223b-4376-b3c1-762780324a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install opencv-python numpy scikit-image scikit-learn matplotlib pandas tensorflow mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb3d1d4-ce5f-4449-8b7b-5154520f5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io \n",
    "import glob  \n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import threading\n",
    "import time\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e90576-c5de-4807-b661-ebaab4886fa3",
   "metadata": {},
   "source": [
    "## Collecting input data to train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5daef-e1a5-458d-83da-4eef9f565e4f",
   "metadata": {},
   "source": [
    "Collecting data for Static Gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf48b93-4287-4497-a767-39c717fd5ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter gesture type (S for Static, D for Dynamic):  S\n",
      "Enter gesture label (A, B, C, D, E, etc.):  C\n",
      "Enter number of continuous samples to collect:  1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 1000 continuous samples for S-C\n",
      "Press 'q' to quit. Press 's' to start data collection.\n",
      "Starting in 2 seconds...\n",
      "Collecting 1000 continuous samples...\n",
      "50/1000 frames collected...\n",
      "100/1000 frames collected...\n",
      "150/1000 frames collected...\n",
      "200/1000 frames collected...\n",
      "250/1000 frames collected...\n",
      "300/1000 frames collected...\n",
      "350/1000 frames collected...\n",
      "400/1000 frames collected...\n",
      "450/1000 frames collected...\n",
      "500/1000 frames collected...\n",
      "550/1000 frames collected...\n",
      "600/1000 frames collected...\n",
      "650/1000 frames collected...\n",
      "700/1000 frames collected...\n",
      "750/1000 frames collected...\n",
      "800/1000 frames collected...\n",
      "850/1000 frames collected...\n",
      "900/1000 frames collected...\n",
      "950/1000 frames collected...\n",
      "1000/1000 frames collected...\n",
      "Finished collecting 1000 samples for S-C\n",
      "Data collection complete. Saved 1000 samples in gesture_data\\S_C.csv.\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "SAVE_DIR = \"gesture_data\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "gesture_type = input(\"Enter gesture type (S for Static): \").strip().upper()\n",
    "gesture_label = input(\"Enter gesture label (A, B, C): \").strip().upper()\n",
    "num_samples = int(input(\"Enter number of continuous samples to collect: \"))\n",
    "\n",
    "print(f\"Collecting {num_samples} continuous samples for {gesture_type}-{gesture_label}\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{gesture_type}_{gesture_label}.csv\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(\"gesture_label,\" + \",\".join([f\"x{i},y{i},z{i}\" for i in range(21)]) + \"\\n\")\n",
    "\n",
    "def save_landmarks(landmarks):\n",
    "    data_row = [gesture_label]\n",
    "    for lm in landmarks:\n",
    "        data_row.extend([lm.x, lm.y, lm.z]) \n",
    "\n",
    "    with open(csv_path, \"a\") as f:\n",
    "        f.write(\",\".join(map(str, data_row)) + \"\\n\")\n",
    "\n",
    "def classify_static(hand_landmarks):\n",
    "    index_up = hand_landmarks[8].y < hand_landmarks[6].y \n",
    "    middle_up = hand_landmarks[12].y < hand_landmarks[10].y  \n",
    "\n",
    "    if index_up and not middle_up:\n",
    "        return \"A\"\n",
    "    elif index_up and middle_up:\n",
    "        return \"B\"\n",
    "    elif not index_up and not middle_up:\n",
    "        return \"C\"\n",
    "    return None \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sample_count = 0\n",
    "prev_landmarks = None\n",
    "collecting = False \n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    print(\"Press 'q' to quit. Press 's' to start data collection.\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        detected_gesture = None\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                landmarks = hand_landmarks.landmark\n",
    "\n",
    "                if gesture_type == \"S\":\n",
    "                    detected_gesture = classify_static(landmarks)\n",
    "                \n",
    "                color = (0, 255, 0) if detected_gesture else (0, 0, 255)\n",
    "                cv2.putText(frame, f\"Gesture: {detected_gesture if detected_gesture else 'None'}\",\n",
    "                            (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('s') and not collecting:\n",
    "                    print(\"Starting in 2 seconds...\")\n",
    "                    time.sleep(2)\n",
    "                    collecting = True\n",
    "                    sample_count = 0\n",
    "                    print(f\"Collecting {num_samples} continuous samples...\")\n",
    "\n",
    "                if collecting and sample_count < num_samples:\n",
    "                    save_landmarks(landmarks)\n",
    "                    sample_count += 1\n",
    "                    if sample_count % 50 == 0:\n",
    "                        print(f\"{sample_count}/{num_samples} frames collected...\")\n",
    "\n",
    "                if collecting and sample_count >= num_samples:\n",
    "                    collecting = False\n",
    "                    print(f\"Finished collecting {num_samples} samples for {gesture_type}-{gesture_label}\")\n",
    "                    break\n",
    "\n",
    "        cv2.imshow('Hand Gesture Collection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Data collection complete. Saved {num_samples} samples in {csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94553880-c927-4d86-b475-0d37a234c9dc",
   "metadata": {},
   "source": [
    "Collecting data for dynamic gesture recognition. Using 30 frames as one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d161d0-b104-431e-9556-d194d22d104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "SAVE_DIR = \"gesture_sequences\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "gesture_label = input(\"Enter dynamic gesture label (D, E, etc.): \").strip().upper()\n",
    "num_sequences = int(input(\"Enter number of sequences to collect: \")) \n",
    "sequence_length = int(input(\"Enter number of frames per sequence: \")) \n",
    "print(f\"Collecting {num_sequences} sequences of {sequence_length} frames for '{gesture_label}' gesture.\")\n",
    "\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{gesture_label}.csv\")\n",
    "if not os.path.exists(csv_path):\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        header = [\"sequence_id\", \"frame_id\"] + [f\"x{i},y{i},z{i}\" for i in range(21)]\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "\n",
    "def save_sequence(sequence_id, frame_id, landmarks):\n",
    "    data_row = [sequence_id, frame_id]  \n",
    "    for lm in landmarks:\n",
    "        data_row.extend([lm.x, lm.y, lm.z]) \n",
    "\n",
    "    with open(csv_path, \"a\") as f:\n",
    "        f.write(\",\".join(map(str, data_row)) + \"\\n\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "    print(\"Press 's' to start collecting data. Press 'q' to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        cv2.putText(frame, \"Press 's' to start recording\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        cv2.imshow('Dynamic Gesture Collection', frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s'):\n",
    "            print(\"Starting in 2 seconds...\")\n",
    "            time.sleep(2)\n",
    "            break\n",
    "        elif key == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit()\n",
    "\n",
    "    sequence_id = 0  \n",
    "    \n",
    "    while sequence_id < num_sequences:\n",
    "        frame_count = 0 \n",
    "\n",
    "        while frame_count < sequence_length:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue        \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    landmarks = hand_landmarks.landmark\n",
    "                    save_sequence(sequence_id, frame_count, landmarks)\n",
    "                    frame_count += 1\n",
    "            \n",
    "            cv2.putText(frame, f\"Recording Sequence {sequence_id + 1}/{num_sequences}\",\n",
    "                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.imshow('Dynamic Gesture Collection', frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "\n",
    "        sequence_id += 1 \n",
    "print(f\"Finished collecting {num_sequences} sequences for {gesture_label}.\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Data collection complete. Data saved in {csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe9a47-68c7-4326-a0ae-fd416623e073",
   "metadata": {},
   "source": [
    "## Training models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503982a-84e5-4870-859e-bef76a82dd70",
   "metadata": {},
   "source": [
    "Loading the saved csv file, and creating the Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6365ef79-6001-4e0f-8690-5abc518265d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3000, 21, 3)\n",
      "y shape: (3000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_A = pd.read_csv(\"gesture_data/S_A.csv\")\n",
    "df_B = pd.read_csv(\"gesture_data/S_B.csv\")\n",
    "df_C = pd.read_csv(\"gesture_data/S_C.csv\")\n",
    "\n",
    "df = pd.concat([df_A, df_B, df_C], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "label_mapping = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
    "df.iloc[:, 0] = df.iloc[:, 0].map(label_mapping)\n",
    "\n",
    "y = df.iloc[:, 0].values\n",
    "X = df.iloc[:, 1:].values\n",
    "\n",
    "num_landmarks = 21  \n",
    "num_coordinates = 3  \n",
    "\n",
    "expected_features = num_landmarks * num_coordinates  \n",
    "actual_features = X.shape[1]\n",
    "\n",
    "if actual_features != expected_features:\n",
    "    raise ValueError(f\"Expected {expected_features} features, but got {actual_features}\")\n",
    "\n",
    "X = X.reshape(X.shape[0], num_landmarks, num_coordinates)\n",
    "\n",
    "y = keras.utils.to_categorical(y, num_classes=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692a288-3aa6-4f68-8491-5759add5af60",
   "metadata": {},
   "source": [
    "Used a CNN to exploit the spatial relation between the 21 points. Saved model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4019a098-5682-4978-a127-202811f17386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taran\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4815 - loss: 1.0030 - val_accuracy: 0.8433 - val_loss: 0.4742\n",
      "Epoch 2/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8560 - loss: 0.4242 - val_accuracy: 0.8867 - val_loss: 0.2669\n",
      "Epoch 3/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9038 - loss: 0.2586 - val_accuracy: 0.9200 - val_loss: 0.2037\n",
      "Epoch 4/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9174 - loss: 0.2043 - val_accuracy: 0.9367 - val_loss: 0.1652\n",
      "Epoch 5/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9400 - loss: 0.1570 - val_accuracy: 0.9267 - val_loss: 0.1903\n",
      "Epoch 6/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9320 - loss: 0.1706 - val_accuracy: 0.9417 - val_loss: 0.1473\n",
      "Epoch 7/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9489 - loss: 0.1381 - val_accuracy: 0.9450 - val_loss: 0.1369\n",
      "Epoch 8/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9481 - loss: 0.1356 - val_accuracy: 0.9467 - val_loss: 0.1460\n",
      "Epoch 9/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.1263 - val_accuracy: 0.9617 - val_loss: 0.1183\n",
      "Epoch 10/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1172 - val_accuracy: 0.9450 - val_loss: 0.1270\n",
      "Epoch 11/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1193 - val_accuracy: 0.9617 - val_loss: 0.1066\n",
      "Epoch 12/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.1018 - val_accuracy: 0.9633 - val_loss: 0.1010\n",
      "Epoch 13/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.0906 - val_accuracy: 0.9717 - val_loss: 0.0915\n",
      "Epoch 14/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.0926 - val_accuracy: 0.9683 - val_loss: 0.0935\n",
      "Epoch 15/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9731 - loss: 0.0795 - val_accuracy: 0.9617 - val_loss: 0.0890\n",
      "Epoch 16/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.0757 - val_accuracy: 0.9733 - val_loss: 0.0838\n",
      "Epoch 17/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.0758 - val_accuracy: 0.9733 - val_loss: 0.0926\n",
      "Epoch 18/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0719 - val_accuracy: 0.9817 - val_loss: 0.0710\n",
      "Epoch 19/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0654 - val_accuracy: 0.9650 - val_loss: 0.0838\n",
      "Epoch 20/20\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0599 - val_accuracy: 0.9717 - val_loss: 0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"gesture_cnn_model.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3136464-7325-4e86-93c4-ee7e04431e64",
   "metadata": {},
   "source": [
    "Loading the locally saved .csv to dataframe and creating Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ccc3bf-d7ab-48f1-be04-0f7df9369f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence_id  frame_id        x0        y0            z0        x1  \\\n",
      "0            0         0  0.194402  0.697508  3.612226e-07  0.253598   \n",
      "1            0         1  0.183029  0.705346  4.700153e-07  0.252113   \n",
      "2            0         2  0.178611  0.707601  4.641686e-07  0.251979   \n",
      "3            0         3  0.175116  0.709159  4.515502e-07  0.248226   \n",
      "4            0         4  0.174221  0.710845  4.316040e-07  0.248648   \n",
      "\n",
      "         y1        z1        x2        y2  ...       z17       x18       y18  \\\n",
      "0  0.687416 -0.010835  0.301478  0.611149  ... -0.039869  0.274102  0.400163   \n",
      "1  0.690928 -0.014209  0.304526  0.618035  ... -0.032517  0.270675  0.402734   \n",
      "2  0.691607 -0.013351  0.304803  0.620390  ... -0.032343  0.273874  0.399957   \n",
      "3  0.691573 -0.013077  0.302036  0.622272  ... -0.033773  0.274288  0.395755   \n",
      "4  0.691815 -0.011558  0.300989  0.621751  ... -0.034071  0.273905  0.397119   \n",
      "\n",
      "        z18       x19       y19       z19       x20       y20       z20  \n",
      "0 -0.061841  0.309287  0.356209 -0.071294  0.345336  0.320142 -0.076099  \n",
      "1 -0.050875  0.303459  0.358630 -0.058835  0.336591  0.321155 -0.063163  \n",
      "2 -0.050570  0.307628  0.355862 -0.059412  0.341907  0.319706 -0.064772  \n",
      "3 -0.052251  0.309474  0.351350 -0.061163  0.345554  0.314908 -0.066584  \n",
      "4 -0.052887  0.308769  0.352602 -0.061691  0.344651  0.315985 -0.066925  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "DATA_DIR = \"gesture_sequences\"\n",
    "\n",
    "sequence_length = 30 \n",
    "num_landmarks = 21 * 3  \n",
    "\n",
    "X, y = [], []\n",
    "gesture_labels = [] \n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".csv\"):\n",
    "        label = file.split(\".\")[0]  \n",
    "        gesture_labels.append(label)\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, file))\n",
    "\n",
    "        grouped = df.groupby(\"sequence_id\")\n",
    "\n",
    "        for _, sequence in grouped:\n",
    "            if len(sequence) == sequence_length:\n",
    "                X.append(sequence.iloc[:, 2:].values)\n",
    "                y.append(label)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c902d-8418-46ea-9832-3c1e35d536ff",
   "metadata": {},
   "source": [
    "Using an LSTM as for dynamic gesture recognition we need to keep track of past frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e3ec7d-81d1-4c96-8199-f3509ff1d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taran\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.5386 - loss: 0.7112 - val_accuracy: 0.6000 - val_loss: 0.6605\n",
      "Epoch 2/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6565 - loss: 0.6448 - val_accuracy: 0.7750 - val_loss: 0.5586\n",
      "Epoch 3/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7813 - loss: 0.5383 - val_accuracy: 0.8750 - val_loss: 0.3823\n",
      "Epoch 4/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9605 - loss: 0.3199 - val_accuracy: 1.0000 - val_loss: 0.0982\n",
      "Epoch 5/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9715 - loss: 0.1168 - val_accuracy: 1.0000 - val_loss: 0.0254\n",
      "Epoch 6/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9870 - loss: 0.0599 - val_accuracy: 1.0000 - val_loss: 0.0089\n",
      "Epoch 7/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9901 - loss: 0.0465 - val_accuracy: 1.0000 - val_loss: 0.0045\n",
      "Epoch 8/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9949 - loss: 0.0310 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 9/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9439 - loss: 0.2336 - val_accuracy: 0.9250 - val_loss: 0.1482\n",
      "Epoch 10/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9611 - loss: 0.0665 - val_accuracy: 0.9750 - val_loss: 0.1080\n",
      "Epoch 11/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9311 - loss: 0.2688 - val_accuracy: 1.0000 - val_loss: 0.0201\n",
      "Epoch 12/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9888 - loss: 0.0494 - val_accuracy: 1.0000 - val_loss: 0.0122\n",
      "Epoch 13/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9773 - loss: 0.0810 - val_accuracy: 1.0000 - val_loss: 0.0178\n",
      "Epoch 14/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9785 - loss: 0.0992 - val_accuracy: 1.0000 - val_loss: 0.0157\n",
      "Epoch 15/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9966 - loss: 0.0265 - val_accuracy: 1.0000 - val_loss: 0.0136\n",
      "Epoch 16/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9949 - loss: 0.0288 - val_accuracy: 1.0000 - val_loss: 0.0116\n",
      "Epoch 17/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9922 - loss: 0.0364 - val_accuracy: 1.0000 - val_loss: 0.0096\n",
      "Epoch 18/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9979 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
      "Epoch 19/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9966 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0066\n",
      "Epoch 20/20\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9870 - loss: 0.0253 - val_accuracy: 1.0000 - val_loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as 'gesture_lstm_model.h5'.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(sequence_length, num_landmarks)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(gesture_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "model.save(\"gesture_lstm_model.h5\")\n",
    "np.save(\"gesture_labels.npy\", encoder.classes_)\n",
    "\n",
    "print(\"Training complete! Model saved as 'gesture_lstm_model.h5'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74bb33-4416-4f65-bf66-048c2d2135b2",
   "metadata": {},
   "source": [
    "## Setting up live gesture classificaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70801c0-f4c7-479e-940e-e33e31b6b0f9",
   "metadata": {},
   "source": [
    "Firstly, loading up both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babba2db-4247-40c3-9099-f802126b1ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "cnn_model = tf.keras.models.load_model(\"gesture_cnn_model.h5\")\n",
    "lstm_model = tf.keras.models.load_model(\"gesture_lstm_model.h5\")\n",
    "gesture_labels = np.load(\"gesture_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ce741-82a7-4942-97cf-b3fbefb07f30",
   "metadata": {},
   "source": [
    "Setting up variables and mapping to actual gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa54096-8d45-49b3-9adb-e9f66947e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "label_mapping = {0: \"A\", 1: \"B\", 2: \"C\"}\n",
    "dynamic_display_mapping = {\"D\": \"BYE\", \"E\": \"WAVES\"}\n",
    "display_mapping = {\"A\": \";)\", \"B\": \"FIST\", \"C\": \"PEACE\"}\n",
    "\n",
    "\n",
    "landmarks_queue = deque(maxlen=2)\n",
    "dynamic_gesture_sequence = deque(maxlen=30) \n",
    "gesture_label = \"No Gesture\"\n",
    "gesture_type = \"STATIC\"\n",
    "running = True\n",
    "dynamic_cooldown = 15 #to stop the classifier from flcikering between static and dynamic \n",
    "cooldown_counter = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ea1e6-9780-4459-9c9e-608d180e3ab2",
   "metadata": {},
   "source": [
    "Defining function to process the 21 points and corresponding 63 coordinates (21*3) and convert to 1-D Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d60b61-6a9a-4993-986b-a45b6726bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_landmarks(landmarks):\n",
    "    flat_landmarks = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
    "    return flat_landmarks.reshape(1, 21, 3) if flat_landmarks.shape[0] == 63 else None\n",
    "\n",
    "def preprocess_dynamic_landmarks(landmarks):\n",
    "    flat_landmarks = np.array([[lm.x, lm.y, lm.z] for lm in landmarks]).flatten()\n",
    "    if flat_landmarks.shape[0] == 63:\n",
    "        dynamic_gesture_sequence.append(flat_landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75563029-8f86-4c84-947a-5945e03795bb",
   "metadata": {},
   "source": [
    "Defining function to classify gesture as Static or Dynamic. \n",
    "Did this by setting up a treshold for allowed movement and calculating the \"movement\" by taking average of the difference between the 21 current and previous landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df8e8659-afe7-4985-9641-655ca82ac805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gesture(threshold=0.02):  \n",
    "    global gesture_type, cooldown_counter\n",
    "    if cooldown_counter > 0:\n",
    "        cooldown_counter -= 1\n",
    "        return  \n",
    "    \n",
    "    if len(landmarks_queue) < 2:\n",
    "        return  \n",
    "    prev_landmarks, curr_landmarks = landmarks_queue\n",
    "    movement = np.mean([\n",
    "    np.linalg.norm(np.array([curr.x, curr.y, curr.z]) - np.array([prev.x, prev.y, prev.z]))\n",
    "    for curr, prev in zip(curr_landmarks, prev_landmarks)\n",
    "])\n",
    "    if movement > threshold:\n",
    "        gesture_type = \"DYNAMIC\"\n",
    "        cooldown_counter = dynamic_cooldown  \n",
    "    else:\n",
    "        gesture_type = \"STATIC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7914200-a1e1-4d4b-a05e-f594181de9a1",
   "metadata": {},
   "source": [
    "Uses the cnn model to classify static gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e309112d-7778-4299-8af5-b70fa5587574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_static_gesture(landmarks):\n",
    "    global gesture_label\n",
    "    X_input = preprocess_landmarks(landmarks)\n",
    "    if X_input is not None:\n",
    "        prediction = cnn_model.predict(X_input, verbose=0)\n",
    "        gesture_label = label_mapping[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ad741-cfe9-45f6-bbf3-2bd9b95f9699",
   "metadata": {},
   "source": [
    "Uses the lstm model to classify dynamic gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f73033-9f17-4447-8d7d-38de236439c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_dynamic_gesture():\n",
    "    global gesture_label\n",
    "    if len(dynamic_gesture_sequence) < 30:\n",
    "        return\n",
    "    X_input = np.array(dynamic_gesture_sequence).reshape(1, 30, 63)\n",
    "    prediction = lstm_model.predict(X_input, verbose=0)\n",
    "    gesture_label = dynamic_display_mapping.get(gesture_labels[np.argmax(prediction)], \"No Gesture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8e8e7-e664-45ed-a96f-d45501b65f15",
   "metadata": {},
   "source": [
    "Capturing live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50e8bd1-459d-496c-999b-9363d1474f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_video():\n",
    "    global running, gesture_label, frame_count\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:\n",
    "        print(\"Press 'q' to quit.\")\n",
    "        while running:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                    landmarks_queue.append(hand_landmarks.landmark)\n",
    "                    preprocess_dynamic_landmarks(hand_landmarks.landmark)\n",
    "                classify_gesture()\n",
    "                if gesture_type == \"STATIC\" and cooldown_counter == 0:\n",
    "                    classify_static_gesture(hand_landmarks.landmark)\n",
    "                elif gesture_type == \"DYNAMIC\":\n",
    "                    classify_dynamic_gesture()\n",
    "            else:\n",
    "                gesture_label = \"No Gesture\"\n",
    "            display_text = display_mapping.get(gesture_label, gesture_label)\n",
    "            cv2.putText(frame, f\"Gesture: {display_text}\", (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if gesture_type == \"STATIC\" else (0, 0, 255), 2)\n",
    "            cv2.imshow('Gesture Recognition', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                running = False\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea841d5f-14dc-450f-b802-233b62a6bde2",
   "metadata": {},
   "source": [
    "Note: Green text -> Static gesture & Red text -> Dynamic gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "602f724e-fb28-4d57-935a-5320cdcfc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_video():\n",
    "    video_thread = threading.Thread(target=capture_video, daemon=True)\n",
    "    video_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "820a1fd8-7cb9-4889-a791-7c595d3f1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_variables():\n",
    "    global running, cooldown_counter, gesture_label, landmarks_queue, dynamic_gesture_sequence\n",
    "    running = True\n",
    "    cooldown_counter = 0\n",
    "    gesture_label = \"No Gesture\"\n",
    "    landmarks_queue.clear()\n",
    "    dynamic_gesture_sequence.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5c4cb-be6d-4a47-88b4-aac60e9816c8",
   "metadata": {},
   "source": [
    "Run this cell to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "285ef831-3925-48a4-ab52-5f9523316350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "reset_variables()\n",
    "start_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mediapipe_env)",
   "language": "python",
   "name": "mediapipe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
